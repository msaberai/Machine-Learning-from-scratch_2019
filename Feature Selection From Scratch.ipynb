{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection methods:\n",
    "\n",
    "Implement and compare different feature selection methods:\n",
    "\n",
    "# This implementation features: \n",
    "   - Correlation Filter (Pearson) \n",
    "   - Mutual information filter\n",
    "   - wrapper method based on KNN \n",
    "   - Base code for an embedding IDEA featuring a NN\n",
    "   - Comparison functions and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import operator\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preperation class \n",
    "class data_prep:                                 #Data preparation and normalization for the model \n",
    "    def __init__(self,train,test):\n",
    "        self.data = open(train, \"r+\")\n",
    "        self.data = self.data.read()\n",
    "        self.data = self.data.splitlines()\n",
    "        self.labels = open(test, \"r+\")\n",
    "        self.labels = self.labels.read()\n",
    "        self.labels = self.labels.splitlines()\n",
    "        for x in range(len(self.data)):\n",
    "            self.data[x] = self.data[x].split(\" \")\n",
    "        for x in range(len(self.data)):\n",
    "            self.data[x] = self.data[x][:-1]\n",
    "                       \n",
    "    def transform(self):\n",
    "        data = self.data\n",
    "        label = self.labels\n",
    "        for w in range(len(data)):\n",
    "            if label[w] == \"-1\":\n",
    "                label[w] = 0\n",
    "            else:\n",
    "                label[w] = 1\n",
    "            for y in range(len(data[w])):\n",
    "                data[w][y] = float(data[w][y]) \n",
    "        self.transformedX = np.array(data)\n",
    "        self.transformedY = np.array(label,ndmin=2).T\n",
    "            \n",
    "    def normalize(self):\n",
    "        features = self.transformedX.T\n",
    "        for y in range(len(features)):\n",
    "            maxi = max(features[y])\n",
    "            mini = min(features[y])\n",
    "            if mini == maxi: \n",
    "                for z in range (len(features[y])):\n",
    "                    features[y][z] =  None  \n",
    "            else:\n",
    "                for z in range (len(features[y])):\n",
    "                    features[y][z] =  (features[y][z]- mini)/ (maxi - mini)\n",
    "        self.normalizedX = features.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data_prep(\"data/FS/arcene_train.data\",\"data/FS/arcene_train.labels\")            \n",
    "train.transform()  \n",
    "train.normalize()\n",
    "\n",
    "test = data_prep(\"data/FS/arcene_valid.data\",\"data/FS/arcene_valid.labels\")            \n",
    "test.transform()      \n",
    "test.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class feature_selector:            \n",
    "    def __init__(self,dataX,dataY,NNMethod):\n",
    "        self.X = dataX\n",
    "        self.Y = dataY.T[0]\n",
    "        self.f = len(self.X.T)    \n",
    "        self.EY = np.mean(self.Y)\n",
    "        self.Ydiff = self.Y - self.EY\n",
    "        self.stdY = np.std(self.Y)\n",
    "        self.NN = NNMethod\n",
    "        \n",
    "    def euclidean(self, basis, example):        # Distance function to calculate \n",
    "        eucl = []                               # the distance for all datapoints.\n",
    "        for v in range(0,len(basis)):\n",
    "            distance = 0 \n",
    "            for y in range(len(example)):\n",
    "                distance = distance + ((basis[v][y]-example[y])**2)\n",
    "            distance**(1/2)\n",
    "            eucl.append(distance)\n",
    "        return np.array(eucl,ndmin=2).T\n",
    "    \n",
    "    def accuracy(self,labels,preds):\n",
    "            return (np.count_nonzero(np.equal(labels,preds))) / len(labels) *100 \n",
    "    \n",
    "    def KNN(self,basis,toPredict): \n",
    "        predictions = []\n",
    "        for x in range(len(toPredict)):\n",
    "            differences = np.array(self.euclidean(basis,toPredict[x])) \n",
    "            idx = np.argmin(differences)\n",
    "            predictions.append(self.Y[idx])\n",
    "        return predictions\n",
    "        \n",
    "    def wrapper_step_forward(self,k,X_test,Y_test):       #wrapper for KNN selection\n",
    "        currentAccuracy = None\n",
    "        listing = []\n",
    "        X_final = False\n",
    "        X_final_test = False\n",
    "        X = self.X.T\n",
    "        X_test = X_test.T\n",
    "        for x in range(k):\n",
    "            best = 0\n",
    "            row = None\n",
    "            if isinstance(X_final,bool) :\n",
    "                for y in range(len(X)): \n",
    "                    predict = self.KNN(np.expand_dims(X[y].T,axis = 1),np.expand_dims(X_test[y].T,axis = 1))\n",
    "                    accuracy = self.accuracy(Y_test.T[0],predict)\n",
    "                    if accuracy > best:\n",
    "                        best = accuracy\n",
    "                        row = y\n",
    "                X_final = np.expand_dims(X[row],axis=1).T\n",
    "                X_final_test = np.expand_dims(X_test[row],axis=1).T\n",
    "                X_test = np.delete(X_test,row,0)\n",
    "                X = np.delete(X,row,0)\n",
    "                listing.append(row)\n",
    "                currentAccuracy = best\n",
    "            else:\n",
    "                for y in range(len(X)): \n",
    "                    data = np.concatenate((X_final, np.expand_dims(X[y],axis = 1).T),axis=0).T\n",
    "                    test = np.concatenate((X_final_test,np.expand_dims(X_test[y],axis = 1).T),axis=0).T\n",
    "                    predict = self.KNN(data,test)\n",
    "                    accuracy = self.accuracy(Y_test.T[0],predict)\n",
    "                    if accuracy > best:\n",
    "                        best = accuracy\n",
    "                        row = y\n",
    "                if best > currentAccuracy: \n",
    "                    X_final = np.concatenate((X_final,np.expand_dims(X[row],axis=1).T),axis=0)\n",
    "                    X_final_test = np.concatenate((X_final_test,np.expand_dims(X_test[row],axis=1).T),axis=0)\n",
    "                    X_test = np.delete(X_test,row,0)\n",
    "                    X = np.delete(X,row,0)\n",
    "                    listing.append(row)\n",
    "                    currentAccuracy = best\n",
    "                else: \n",
    "                    break\n",
    "        print(\"For features \" + str(listing) + \" the best accuracy was achieved.  Accuracy: \"  + str(self.accuracy(Y_test.T[0],self.KNN(X_final.T,X_final_test.T)))+ \"%\")\n",
    "        return  listing  \n",
    "    \n",
    "    def cov(self):          \n",
    "        differenceX = self.X - np.expand_dims(np.mean(self.X,axis = 0),1).T\n",
    "        return np.sum(differenceX.T * self.Ydiff,axis = 1) /(len(self.Y)-1)\n",
    "    \n",
    "    def pearson(self):\n",
    "        covariance = self.cov()\n",
    "        std = np.std(self.X.T,axis=1) \n",
    "        return covariance / std*self.stdY\n",
    "    \n",
    "    def correlation_filter(self,k):        #Feature selection based on Correlation matrix\n",
    "        pearson = self.pearson()\n",
    "        pearson = np.where(np.isnan(pearson),0,pearson)  # hack to get rid of NaNs\n",
    "        selection = np.flip(np.argsort(np.absolute(pearson)))[:k]\n",
    "        print(\"Features \" + str(selection) + \" where selected\")\n",
    "        return selection\n",
    "    \n",
    "    def mutual_information(self,where):  # Information based Feature\n",
    "        distribution = self.X.T[where]\n",
    "        pY = {}\n",
    "        pX = {}\n",
    "        pXY = {}\n",
    "        if np.isnan(self.X.T[where][0]):\n",
    "            return 0\n",
    "        value = 1/len(self.Y)\n",
    "        for x in self.Y:\n",
    "            if x in pY: \n",
    "                pY[x] += value\n",
    "            else: \n",
    "                pY[x] = value\n",
    "        for y in distribution:\n",
    "            if y in pX: \n",
    "                pX[y] += value\n",
    "            else: \n",
    "                pX[y] = value\n",
    "        for z in range(len(self.X)):\n",
    "            pXY[str(1) + \" \" +  str(distribution[z])] = 0 \n",
    "            pXY[str(0) + \" \" +  str(distribution[z])] = 0 \n",
    "        for u in range(len(self.X)):\n",
    "            pXY[str(self.Y[u]) + \" \" +  str(distribution[u])] +=value\n",
    "        I = 0       \n",
    "        for keyY, valueY in pY.items():\n",
    "            for keyX, valueX in pX.items():\n",
    "                a = pXY[str(keyY)+ \" \" + str(keyX)]\n",
    "                if a == 0:\n",
    "                    pass\n",
    "                else:\n",
    "                    I+= a * math.log(a/(valueY*valueX))\n",
    "        return I\n",
    "                        \n",
    "    def find_max_information(self,k):      \n",
    "        scores = []\n",
    "        for x in range(len(self.X[0])):\n",
    "            scores.append(self.mutual_information(x))\n",
    "        scores = np.array(scores)\n",
    "        best = np.flip(np.argsort(scores))[:k]\n",
    "        print(\"Features \" + str(best) + \" where selected\")\n",
    "        return best\n",
    "    \n",
    "    def find_best_emdedded(self,k,lr,epochs,jump,Y_test,load):  #Calls NN Method to select Feature subset\n",
    "        if load == False:\n",
    "            self.embedded  = self.NN.feature_selection(k,lr,epochs,jump)  #SELECT VERSION (_alt)\n",
    "        out = []\n",
    "        for x in range (len(self.embedded)):\n",
    "            prediction = self.KNN(self.embedded[x][0], self.embedded[x][1])\n",
    "            out.append(self.accuracy(Y_test.T[0],prediction))\n",
    "        print(\"Embedded Method Accuracy: \" + str(out))\n",
    "        return out\n",
    "            \n",
    "    def score_selection(self,selection,X_test,Y_test): # Runs KNN wih feature selection\n",
    "        X = np.take(self.X.T, selection,axis=0).T\n",
    "        testX = np.take(X_test.T, selection,axis=0).T\n",
    "        prediction = self.KNN(X,testX)\n",
    "        return self.accuracy(Y_test.T[0],prediction)\n",
    "    \n",
    "    def reconstruct_indices(self,arr): #FIX \n",
    "        new = []\n",
    "        for x in range(len(arr)): \n",
    "            element = arr[x]\n",
    "            check = len([i for i in arr[:x] if i < element])\n",
    "            new.append(element+check)\n",
    "        return new\n",
    "            \n",
    "    def feature_selection(self,steps,X_test,Y_test,load):  #Scores all methods and compares\n",
    "        if load == False:\n",
    "            self.filter1 = self.find_max_information(steps[-1])\n",
    "            self.filter2 = self.correlation_filter(steps[-1])\n",
    "            self.wrapper = self.self.wrapper_step_forward(steps[-1],X_test,Y_test)\n",
    "        embeddedScore = self.find_best_emdedded(steps,0.1,20,1,Y_test,load)\n",
    "        #embeddedScore.reverse()\n",
    "        self.wrapper = self.reconstruct_indices(self.wrapper)\n",
    "        results = np.zeros((4,len(steps)))\n",
    "        for x in range(len(steps)): \n",
    "            results[0][x] = self.score_selection(self.filter1[:steps[x]],X_test,Y_test)    \n",
    "            results[1][x] = self.score_selection(self.filter2[:steps[x]],X_test,Y_test)\n",
    "            results[2][x] = self.score_selection(self.wrapper[:steps[x]],X_test,Y_test)\n",
    "            results[3][x] = embeddedScore[x]\n",
    "        if load == False: \n",
    "            self.save_selections()\n",
    "        df_cm = pd.DataFrame(results,columns=steps,index=[\"information\",\"correlation\",\"wrapper\",\"embedded\"])\n",
    "        print(df_cm)\n",
    "    \n",
    "    def save_selections(self):  #save your selection or load preselected Features\n",
    "            pickle_out = open(\"feature_selection/information.pickle\",\"wb\")\n",
    "            pickle.dump(self.filter1, pickle_out)\n",
    "            pickle_out.close()\n",
    "            pickle_out = open(\"feature_selection/correlation.pickle\",\"wb\")\n",
    "            pickle.dump(self.filter2, pickle_out)\n",
    "            pickle_out.close()\n",
    "            pickle_out = open(\"feature_selection/wrapper.pickle\",\"wb\")\n",
    "            pickle.dump(self.wrapper, pickle_out)\n",
    "            pickle_out.close()\n",
    "            pickle_out = open(\"feature_selection/embedding.pickle\",\"wb\")\n",
    "            pickle.dump(self.embedded, pickle_out)\n",
    "            pickle_out.close()\n",
    "           \n",
    "            \n",
    "    def load_selections(self):\n",
    "            pickle_in = open(\"feature_selection/information.pickle\",\"rb\")\n",
    "            self.filter1 = pickle.load(pickle_in)\n",
    "            pickle_in = open(\"feature_selection/correlation.pickle\",\"rb\")\n",
    "            self.filter2 = pickle.load(pickle_in)\n",
    "            pickle_in = open(\"feature_selection/wrapper.pickle\",\"rb\")\n",
    "            self.wrapper = pickle.load(pickle_in)\n",
    "            pickle_in = open(\"feature_selection/embedding.pickle\",\"rb\")\n",
    "            self.embedded = pickle.load(pickle_in)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class feature_selector_embedded_NN:\n",
    "        def __init__(self, NNeurons,X,Y,X_test,Y_test):\n",
    "            self.shape = NNeurons\n",
    "            self.X, self.y = np.where(np.isnan(X.T),0,X.T),Y.T[0]\n",
    "            self.X_test, self.y_test = X_test.T,Y_test.T[0]\n",
    "            self.input = self.X.shape[0]\n",
    "            self.output = np.max(self.y)+1\n",
    "            self.sizeX = self.X.shape[1]\n",
    "            self.sizeY = self.y.shape[0]\n",
    "            self.weights,self.bias = self.initialize_NN(NNeurons)\n",
    "            \n",
    "        def initialize_NN(self,NN):  #initialize all the weights based on parameters\n",
    "            weights = {}\n",
    "            bias = {}\n",
    "            weights[0] = np.random.normal(0,0.1,(NN[0], self.input))\n",
    "            bias[0] = np.random.normal(0,0.1,(NN[0], 1))\n",
    "            for x in range(1,len(NN)):\n",
    "                weights[x] = np.random.normal(0,0.1,(NN[x], NN[x-1]))\n",
    "                bias[x] = np.random.normal(0,0.1,(NN[x], 1))\n",
    "            weights[len(NN)] = np.random.normal(0,0.1,(self.output, NN[len(NN)-1]))\n",
    "            bias[len(NN)] = np.random.normal(0,0.1,(self.output, 1))\n",
    "            return weights,bias\n",
    "        \n",
    "        def softmax(self,X):  #Activation functions and derivatives\n",
    "            X = X-(np.amax(X,axis=0))\n",
    "            return np.divide(np.exp(X),np.sum( np.exp(X), axis=0,keepdims = True))\n",
    "        \n",
    "        def relu(self,Z):\n",
    "            return np.where(Z<0,0,Z)\n",
    "        \n",
    "        def relu_dev(self,Z):\n",
    "            return np.where(Z<0,0,1)\n",
    "            \n",
    "        def forward(self,inputs):  #forward step (full data matrics is forwarded at once)\n",
    "            caches = []\n",
    "            cache = []\n",
    "            out = np.dot(self.weights[0],inputs) + self.bias[0]\n",
    "            cache.append(out)\n",
    "            out = self.relu(out)\n",
    "            cache.append(out)\n",
    "            caches.append(cache)\n",
    "            for x in range (1,len(self.weights)-1):\n",
    "                cache = []\n",
    "                out = np.dot(self.weights[x],out) + self.bias[x]\n",
    "                cache.append(out)\n",
    "                out = self.relu(out)\n",
    "                cache.append(out)\n",
    "                caches.append(cache)\n",
    "            out = np.dot(self.weights[len(self.weights)-1],out) + self.bias[len(self.weights)-1]\n",
    "            cache = []\n",
    "            cache.append(out)\n",
    "            out = self.softmax(out)\n",
    "            cache.append(out)\n",
    "            caches.append(cache)\n",
    "            return out, np.array(caches)\n",
    "        \n",
    "        def backward(self,cache,lr): #backward step based on full dataset (update included)\n",
    "            dWs = []\n",
    "            dBs = []\n",
    "            y = self.oneHotEncoding(self.y)\n",
    "            dZ = cache[len(cache)-1][1]- y\n",
    "            dW = np.dot(dZ, cache[len(cache)-2][1].T)/self.sizeX\n",
    "            dB = np.sum(dZ, axis=1, keepdims=True)/self.sizeX\n",
    "            dWs.append( lr * dW)\n",
    "            dBs.append( lr * dB)\n",
    "            for x in range (len(cache)-1,1,-1): \n",
    "                dZ = np.dot(self.weights[x].T,dZ)*self.relu_dev(cache[x-1][0])\n",
    "                dW = np.dot(dZ, cache[x-2][1].T)/self.sizeX\n",
    "                dB = np.sum(dZ, axis=1, keepdims=True)/self.sizeX\n",
    "                dWs.append( lr * dW)\n",
    "                dBs.append( lr * dB) \n",
    "            dZ = np.dot(self.weights[1].T,dZ)*self.relu_dev(cache[0][0])\n",
    "            dW = np.dot(dZ, self.X.T)/self.sizeX\n",
    "            dOut = np.dot(self.weights[0].T,dZ)\n",
    "            dB = np.sum(dZ, axis=1, keepdims=True)/self.sizeX\n",
    "            dWs.append( lr * dW)\n",
    "            dBs.append( lr * dB)\n",
    "            for y in range(len(dWs)): \n",
    "                self.weights[y] = self.weights[y]- dWs[len(dWs)-1-y]\n",
    "                self.bias[y] = self.bias[y]- dBs[len(dWs)-1-y]\n",
    "            self.outGrad = dOut\n",
    "                    \n",
    "        def oneHotEncoding(self,label): #transform the labels into vectors\n",
    "            encode = np.zeros((self.output,len(label)))\n",
    "            for z in range(len(label)):\n",
    "                encode[self.y[z]][z] = 1\n",
    "            return encode\n",
    "        \n",
    "        def log_loss(self,result): #loss function\n",
    "            loss = self.oneHotEncoding(self.y)* np.log(result) /self.sizeX\n",
    "            return -np.sum(loss)\n",
    "        \n",
    "        def find_pred(self,probs):   #select the number with the highest probability\n",
    "            return np.argmax(probs,axis=0)\n",
    "        \n",
    "        def confusion_matrix(self,labels,preds): #produce confusion matrix\n",
    "            confusion = np.zeros((self.output,self.output))\n",
    "            for x in range(len(preds)):\n",
    "                confusion[int(labels[x])][int(preds[x])] +=1\n",
    "            return confusion\n",
    "        \n",
    "        def accuracy(self,labels,preds): #accuracy score\n",
    "            return (np.count_nonzero(np.equal(labels,preds))) / len(labels) *100  \n",
    "        \n",
    "        def predict(self,what,heatmap): #predict either the whole train or test data and display\n",
    "            if what == \"train\": # the metrics of the predictions\n",
    "                prob,cache = self.forward(self.X)\n",
    "                labels = self.oneHotEncoding(self.y)\n",
    "            elif what == \"test\":\n",
    "                prob,cache = self.forward(self.X_test)\n",
    "                labels = self.oneHotEncoding(self.y_test)\n",
    "            predictions = self.find_pred(prob)\n",
    "            if what == \"train\":\n",
    "                CM = self.confusion_matrix(self.y,predictions)\n",
    "                Accuracy = self.accuracy(self.y,predictions)\n",
    "            if what == \"test\":\n",
    "                CM = self.confusion_matrix(self.y_test,predictions)\n",
    "                Accuracy = self.accuracy(self.y_test,predictions) \n",
    "            print(\"Accuracy with \" + str(self.input) + \" features: \" +  str(Accuracy)+ \"%\")\n",
    "            if heatmap == True:\n",
    "                df_cm = pd.DataFrame(CM)\n",
    "                plt.figure(figsize = (10,7))\n",
    "                ax = plt.axes()\n",
    "                ax.set_title('True Labels = Rows, Predictions = Columms')\n",
    "                sn.set(font_scale=1)\n",
    "                sn.heatmap(df_cm, annot=True,cbar = False)\n",
    "            \n",
    "        def train(self,lr,iterations):  #Loop over forward and backward as long as you want\n",
    "            for x in range(iterations):\n",
    "                result,cache = self.forward(self.X)\n",
    "                self.backward(cache,lr)\n",
    "                \n",
    "        def feature_selection(self,maxFeatures,lr,epochs,k):\n",
    "            selections = []\n",
    "            for x in range(0,self.input-maxFeatures[0],k):\n",
    "                self.train(lr,epochs)\n",
    "                self.predict(\"train\",False)\n",
    "                strength =  np.sum(np.abs(self.weights[0]),axis=0)\n",
    "                worst = np.argsort(strength)[:k]\n",
    "                self.X = np.delete(self.X,worst,0)\n",
    "                self.X_test = np.delete(self.X_test,worst,0)\n",
    "                self.input = self.input - k\n",
    "                if self.input in maxFeatures: \n",
    "                    selections.append([self.X.T, self.X_test.T])\n",
    "                self.weights,self.bias = self.initialize_NN(self.shape)\n",
    "            return selections\n",
    "        \n",
    "        def feature_selection_alt(self,maxFeatures,lr,epochs,k):\n",
    "            selections = []\n",
    "            self.train(lr,epochs)\n",
    "            self.predict(\"train\",False)\n",
    "            strength =  np.sum(np.abs(self.weights[0]),axis=0)\n",
    "            best = np.flip(np.argsort(strength))\n",
    "            for x in maxFeatures:\n",
    "                X = np.take(self.X, best[:x],axis=0).T\n",
    "                X_test = np.take(self.X_test, best[:x],axis=0).T\n",
    "                selections.append([X, X_test])\n",
    "            selections.reverse()\n",
    "            return selections\n",
    "        \n",
    "        def feature_selection_alt2(self,maxFeatures,lr,epochs,k):\n",
    "            selections = []\n",
    "            self.train(lr,epochs)\n",
    "            self.predict(\"train\",False)\n",
    "            strength =  np.sum(np.absolute(self.outGrad),axis=1)\n",
    "            best = np.flip(np.argsort(strength))\n",
    "            for x in maxFeatures:\n",
    "                X = np.take(self.X, best[:x],axis=0).T\n",
    "                X_test = np.take(self.X_test, best[:x],axis=0).T\n",
    "                selections.append([X, X_test])\n",
    "            selections.reverse()\n",
    "            return selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded Method Accuracy: [56.00000000000001, 64.0, 63.0]\n",
      "                1     2     5\n",
      "information  49.0  51.0  68.0\n",
      "correlation  63.0  62.0  58.0\n",
      "wrapper      77.0  81.0  89.0\n",
      "embedded     56.0  64.0  63.0\n"
     ]
    }
   ],
   "source": [
    "# Display preselected Features and their performance\n",
    "\n",
    "NN = feature_selector_embedded_NN([500],train.transformedX,train.transformedY,test.transformedX,test.transformedY)\n",
    "selector = feature_selector(train.normalizedX,train.transformedY,NN)\n",
    "selector.load_selections()\n",
    "selector.feature_selection([1,2,5],test.transformedX,test.transformedY,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all methods and display result\n",
    "\n",
    "NN = feature_selector_embedded_NN([500],train.transformedX,train.transformedY,test.transformedX,test.transformedY)\n",
    "selector = feature_selector(train.normalizedX,train.transformedY,NN)\n",
    "selector.feature_selection([1,2,5],test.transformedX,test.transformedY,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with 10000 features: 97.0%\n",
      "Accuracy with 9999 features: 92.0%\n",
      "Accuracy with 9998 features: 88.0%\n",
      "Accuracy with 9997 features: 87.0%\n",
      "Accuracy with 9996 features: 97.0%\n",
      "Accuracy with 9995 features: 95.0%\n",
      "Accuracy with 9994 features: 96.0%\n",
      "Accuracy with 9993 features: 79.0%\n",
      "Accuracy with 9992 features: 91.0%\n",
      "Accuracy with 9991 features: 93.0%\n",
      "Accuracy with 9990 features: 96.0%\n",
      "Accuracy with 9989 features: 96.0%\n",
      "Accuracy with 9988 features: 79.0%\n",
      "Accuracy with 9987 features: 92.0%\n",
      "Accuracy with 9986 features: 98.0%\n",
      "Accuracy with 9985 features: 99.0%\n",
      "Accuracy with 9984 features: 95.0%\n",
      "Accuracy with 9983 features: 96.0%\n",
      "Accuracy with 9982 features: 94.0%\n",
      "Accuracy with 9981 features: 90.0%\n",
      "Accuracy with 9980 features: 98.0%\n",
      "Accuracy with 9979 features: 81.0%\n",
      "Accuracy with 9978 features: 92.0%\n",
      "Accuracy with 9977 features: 99.0%\n",
      "Accuracy with 9976 features: 91.0%\n",
      "Accuracy with 9975 features: 80.0%\n",
      "Accuracy with 9974 features: 46.0%\n",
      "Accuracy with 9973 features: 95.0%\n",
      "Accuracy with 9972 features: 98.0%\n",
      "Accuracy with 9971 features: 98.0%\n",
      "Accuracy with 9970 features: 96.0%\n",
      "Accuracy with 9969 features: 90.0%\n",
      "Accuracy with 9968 features: 86.0%\n",
      "Accuracy with 9967 features: 98.0%\n",
      "Accuracy with 9966 features: 97.0%\n",
      "Accuracy with 9965 features: 95.0%\n",
      "Accuracy with 9964 features: 84.0%\n",
      "Accuracy with 9963 features: 98.0%\n",
      "Accuracy with 9962 features: 98.0%\n",
      "Accuracy with 9961 features: 97.0%\n",
      "Accuracy with 9960 features: 96.0%\n",
      "Accuracy with 9959 features: 59.0%\n",
      "Accuracy with 9958 features: 98.0%\n",
      "Accuracy with 9957 features: 94.0%\n",
      "Accuracy with 9956 features: 94.0%\n",
      "Accuracy with 9955 features: 88.0%\n",
      "Accuracy with 9954 features: 96.0%\n",
      "Accuracy with 9953 features: 96.0%\n",
      "Accuracy with 9952 features: 94.0%\n",
      "Accuracy with 9951 features: 99.0%\n",
      "Accuracy with 9950 features: 76.0%\n",
      "Accuracy with 9949 features: 97.0%\n",
      "Accuracy with 9948 features: 90.0%\n",
      "Accuracy with 9947 features: 79.0%\n",
      "Accuracy with 9946 features: 99.0%\n",
      "Accuracy with 9945 features: 95.0%\n",
      "Accuracy with 9944 features: 97.0%\n",
      "Accuracy with 9943 features: 89.0%\n",
      "Accuracy with 9942 features: 93.0%\n",
      "Accuracy with 9941 features: 65.0%\n",
      "Accuracy with 9940 features: 99.0%\n",
      "Accuracy with 9939 features: 94.0%\n",
      "Accuracy with 9938 features: 95.0%\n",
      "Accuracy with 9937 features: 78.0%\n",
      "Accuracy with 9936 features: 73.0%\n",
      "Accuracy with 9935 features: 93.0%\n",
      "Accuracy with 9934 features: 84.0%\n",
      "Accuracy with 9933 features: 66.0%\n",
      "Accuracy with 9932 features: 95.0%\n",
      "Accuracy with 9931 features: 93.0%\n",
      "Accuracy with 9930 features: 80.0%\n",
      "Accuracy with 9929 features: 75.0%\n",
      "Accuracy with 9928 features: 98.0%\n",
      "Accuracy with 9927 features: 95.0%\n",
      "Accuracy with 9926 features: 89.0%\n",
      "Accuracy with 9925 features: 96.0%\n",
      "Accuracy with 9924 features: 95.0%\n",
      "Accuracy with 9923 features: 93.0%\n",
      "Accuracy with 9922 features: 98.0%\n",
      "Accuracy with 9921 features: 91.0%\n",
      "Accuracy with 9920 features: 82.0%\n",
      "Accuracy with 9919 features: 91.0%\n",
      "Accuracy with 9918 features: 93.0%\n",
      "Accuracy with 9917 features: 69.0%\n",
      "Accuracy with 9916 features: 95.0%\n",
      "Accuracy with 9915 features: 98.0%\n",
      "Accuracy with 9914 features: 97.0%\n",
      "Accuracy with 9913 features: 96.0%\n",
      "Accuracy with 9912 features: 92.0%\n",
      "Accuracy with 9911 features: 98.0%\n",
      "Accuracy with 9910 features: 63.0%\n",
      "Accuracy with 9909 features: 66.0%\n",
      "Accuracy with 9908 features: 96.0%\n",
      "Accuracy with 9907 features: 94.0%\n",
      "Accuracy with 9906 features: 95.0%\n",
      "Accuracy with 9905 features: 92.0%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-178-db4a282531c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mNN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_selector_embedded_NN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformedX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformedY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformedX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformedY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mselector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_selector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalizedX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformedY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_selection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mselector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore_selection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformedX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformedY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-177-24ee401de337>\u001b[0m in \u001b[0;36mfeature_selection\u001b[1;34m(self, maxFeatures, lr, epochs, k)\u001b[0m\n\u001b[0;32m    134\u001b[0m             \u001b[0mselections\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mmaxFeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m                 \u001b[0mstrength\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-177-24ee401de337>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, lr, iterations)\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfeature_selection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmaxFeatures\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-177-24ee401de337>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, cache, lr)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[0mdZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu_dev\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[0mdW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msizeX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mdOut\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m             \u001b[0mdB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msizeX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[0mdWs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#TestStation for Embedding method\n",
    "\n",
    "NN = feature_selector_embedded_NN([100],train.transformedX,train.transformedY,test.transformedX,test.transformedY)\n",
    "selector = feature_selector(train.normalizedX,train.transformedY,NN)\n",
    "x = selector.NN.feature_selection([10],0.1,20,1)\n",
    "selector.score_selection(x,test.transformedX,test.transformedY)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
